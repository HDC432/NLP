{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HDC432/NLP/blob/main/cross_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Language Retrieval\n",
        "\n",
        "In this notebook, you will evaluate models on the task of cross-language retrieval. We will use a sample of the first paragraphs of Wikipedia articles. Sometimes, a Wikipedia article in one language will be a translation of the article in another; in other cases, articles cover the some topic but are not translations. In any case, we use the links between Wikipedia articles in different languages as ground truth for our evaluation.\n",
        "\n",
        "Since we often want to enrich the context information available to a language model with retrieval results, we will evaluate not only whether the exact matching document ranks highest, but also whether the matching document ranks in the top $k$.\n",
        "\n",
        "Work through the notebook and complete code and text cells marked **TODO**.\n",
        "\n",
        "We start by installing the `sentence-transformers` library."
      ],
      "metadata": {
        "id": "Js7ZK5vrn4bG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bxAyQ67UPpQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7dd21c8-9467-44dd-c3a3-90c56f52fe4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then download a sample of the first paragraphs of Wikipedia articles in six languages."
      ],
      "metadata": {
        "id": "G6hetLMwp12W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/dasmiq/cs6120-assignment5/refs/heads/main/sample-6lang.jsonl"
      ],
      "metadata": {
        "id": "CqtwbxhRVN9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6330c8-e0b4-4e9f-95dd-303a38da31bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-07 19:21:06--  https://raw.githubusercontent.com/dasmiq/cs6120-assignment5/refs/heads/main/sample-6lang.jsonl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7514418 (7.2M) [text/plain]\n",
            "Saving to: ‘sample-6lang.jsonl’\n",
            "\n",
            "sample-6lang.jsonl  100%[===================>]   7.17M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-12-07 19:21:06 (133 MB/s) - ‘sample-6lang.jsonl’ saved [7514418/7514418]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "articles = []\n",
        "\n",
        "for line in open('sample-6lang.jsonl', mode='r', encoding='utf-8'):\n",
        "  rec = json.loads(line)\n",
        "  articles.append(rec)\n",
        "\n",
        "len(articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvUYbqqOUyw0",
        "outputId": "6c6cd16f-8a0e-4d98-9f28-7ade0eb1b7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11838"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We include articles from the three most prevalent Wikipedia languages—English, German, and French—and from three other languages in non-Latin scripts—Chinese, Arabic, and Greek. The dataset includes fields for the `text` of the paragraph, as well as the (lower-cased) `title` of the article and `lang` for the language code. Finally, each record contains the Wikidata `id` used to link related articles in different languages. For convenience, the records have been sorted by `id` and `lang`.\n",
        "\n",
        "If you read a few of these languages (or translate them), you can look at a set of paragraphs and see that most pairs are not translations of each other."
      ],
      "metadata": {
        "id": "gDxL8mJGqB5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles[6:12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TFQlLz0WQgL",
        "outputId": "7ad6e3cf-6f76-4e39-de9a-ebc4fbd1ee18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'Q1005289',\n",
              "  'lang': 'ar',\n",
              "  'title': 'قانون الجنسية الكندي',\n",
              "  'url': 'https://ar.wikipedia.org/wiki/%D9%82%D8%A7%D9%86%D9%88%D9%86%20%D8%A7%D9%84%D8%AC%D9%86%D8%B3%D9%8A%D8%A9%20%D8%A7%D9%84%D9%83%D9%86%D8%AF%D9%8A',\n",
              "  'text': 'قانون الجنسية الكندي، يشار إليها أيضًا بالجنسية الكندية، هو وضع قانوني يمنح الشخص الطبيعي حقوقًا ومسؤوليات محددة في كندا. نشأ في عام ، وصار معلمًا هامًا في عملية استقلال كندا عن المملكة المتحدة مع دخول قانون الجنسية الكندية الأول حيز التنفيذ. تخضع الجنسية الكندية الآن لقانون الجنسية لعام 1977، الذي خضع لعدة تعديلات مهمة منذ دخوله حيز التنفيذ. كما ساهمت المحاكم الفيدرالية، من خلال قانونها القضائي، في توضيح التعريف القانوني للجنسية الكندية.'},\n",
              " {'id': 'Q1005289',\n",
              "  'lang': 'de',\n",
              "  'title': 'kanadische staatsangehörigkeit',\n",
              "  'url': 'https://de.wikipedia.org/wiki/Kanadische%20Staatsangeh%C3%B6rigkeit',\n",
              "  'text': 'Die kanadische Staatsbürgerschaft ( bzw. Canadian Citizenship) ist die Staatsbürgerschaft Kanadas, die im engeren Sinne seit 1947 existiert.'},\n",
              " {'id': 'Q1005289',\n",
              "  'lang': 'el',\n",
              "  'title': 'νόμος περί καναδικής ιθαγένειας',\n",
              "  'url': 'https://el.wikipedia.org/wiki/%CE%9D%CF%8C%CE%BC%CE%BF%CF%82%20%CF%80%CE%B5%CF%81%CE%AF%20%CE%9A%CE%B1%CE%BD%CE%B1%CE%B4%CE%B9%CE%BA%CE%AE%CF%82%20%CE%B9%CE%B8%CE%B1%CE%B3%CE%AD%CE%BD%CE%B5%CE%B9%CE%B1%CF%82',\n",
              "  'text': 'Η καναδική ιθαγένεια (αγγλικά: Canadian nationality· γαλλικά: Nationalité canadienne) ρυθμίζεται από τον νόμο Νόμο Περί Υπηκοότητας (Citizenship Act) από το 1977. Ο νόμος καθορίζει ποιος είναι, ή είναι επιλέξιμος να είναι, πολίτης του Καναδά. Αφού αντικατέστησε τον προηγούμενο Νόμο Περί Καναδικής Υπηκοότητας το 1977, ο νόμος πέρασε από τέσσερεις σημαντικές τροποποιήσεις, το 2007, 2009, 2015, και το 2017.'},\n",
              " {'id': 'Q1005289',\n",
              "  'lang': 'en',\n",
              "  'title': 'canadian nationality law',\n",
              "  'url': 'https://en.wikipedia.org/wiki/Canadian%20nationality%20law',\n",
              "  'text': 'Canadian nationality law details the conditions by which a person is a national of Canada. The primary law governing these regulations is the Citizenship Act, which came into force on February 15, 1977 and is applicable to all provinces and territories of Canada.'},\n",
              " {'id': 'Q1005289',\n",
              "  'lang': 'fr',\n",
              "  'title': 'citoyenneté canadienne',\n",
              "  'url': 'https://fr.wikipedia.org/wiki/Citoyennet%C3%A9%20canadienne',\n",
              "  'text': 'La citoyenneté canadienne, également désignée comme la nationalité canadienne, est un statut juridique conférant à une personne physique des droits et des responsabilités particuliers au Canada. Sa création en 1947, avec l’entrée en vigueur de la première Loi sur la citoyenneté canadienne, est un jalon important du processus d’indépendance du Canada par rapport au Royaume-Uni. La nationalité canadienne est désormais régie par la Loi sur la citoyenneté de 1977, qui a subi plusieurs amendements importants depuis son entrée en vigueur. Les tribunaux fédéraux ont également contribué, par leur jurisprudence, à préciser la définition légale de la citoyenneté canadienne.'},\n",
              " {'id': 'Q1005289',\n",
              "  'lang': 'zh',\n",
              "  'title': '加拿大國籍法',\n",
              "  'url': 'https://zh.wikipedia.org/wiki/%E5%8A%A0%E6%8B%BF%E5%A4%A7%E5%9C%8B%E7%B1%8D%E6%B3%95',\n",
              "  'text': '加拿大國籍通常是藉著出生於加拿大境內自动获得，或父母至少其中一方是加拿大公民來取得，或者被一位或多位加拿大人收养而得到。此外，在過去五年中在加拿大居住至少三年的永久居民也可获取加拿大國籍。下面会具体说明这些情况的例子和例外之处。'}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load a sentence embedding model, `LaBSE`, that was trained on several languages, including the six we work with here."
      ],
      "metadata": {
        "id": "8hNSDLwrrV35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "labse = SentenceTransformer('sentence-transformers/LaBSE')"
      ],
      "metadata": {
        "id": "OZg5R-GwUsNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate finding similar paragraphs, we encode the text of the first twelve records, which gives us a 768-dimensional embedding vector for each one."
      ],
      "metadata": {
        "id": "C1kJ8kbCsqB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = labse.encode([r['text'] for r in articles[0:12]])\n",
        "encoded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBvdBUphXDX2",
        "outputId": "9973b391-a954-45b5-bef9-0611b5530819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we multiply this $12 \\times 768$ matrix by its transpose, we get a $12 \\times 12$ (symmetric) matrix with the cosine similarity between all pairs of paragraphs. The diagonal entries are, of course, approximately 1. In the first six rows, we can see that the first six columns are higher than the latter six. In the latter six rows, we can see that the latter six columns are higher than the first six."
      ],
      "metadata": {
        "id": "-sn4xhzPuziB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded @ encoded.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds8c1nF1Xlxb",
        "outputId": "b8bfc3cf-5393-4dc4-d206-9543b52cf888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.76256883, 0.7722026 , 0.7502576 , 0.5858218 ,\n",
              "        0.6954646 , 0.37287953, 0.32353517, 0.33085033, 0.2717247 ,\n",
              "        0.32900903, 0.20401588],\n",
              "       [0.76256883, 0.99999994, 0.7690512 , 0.92336786, 0.63490856,\n",
              "        0.65711933, 0.4801091 , 0.4884625 , 0.4031633 , 0.36106873,\n",
              "        0.42383695, 0.26584467],\n",
              "       [0.7722026 , 0.7690512 , 1.0000002 , 0.7562181 , 0.5307793 ,\n",
              "        0.6586367 , 0.3545522 , 0.36299706, 0.35417843, 0.26405597,\n",
              "        0.3212558 , 0.21048519],\n",
              "       [0.7502576 , 0.92336786, 0.7562181 , 0.99999976, 0.6517093 ,\n",
              "        0.6516982 , 0.41325855, 0.4034108 , 0.35402954, 0.3217687 ,\n",
              "        0.3496586 , 0.21814035],\n",
              "       [0.5858218 , 0.63490856, 0.5307793 , 0.6517093 , 1.0000002 ,\n",
              "        0.541567  , 0.3823491 , 0.3980208 , 0.36890098, 0.3174026 ,\n",
              "        0.33768153, 0.28169265],\n",
              "       [0.6954646 , 0.65711933, 0.6586367 , 0.6516982 , 0.541567  ,\n",
              "        0.9999998 , 0.27841944, 0.29993683, 0.30385193, 0.21876109,\n",
              "        0.2602014 , 0.20053254],\n",
              "       [0.37287953, 0.4801091 , 0.3545522 , 0.41325855, 0.3823491 ,\n",
              "        0.27841944, 0.9999999 , 0.6758275 , 0.7730411 , 0.69483066,\n",
              "        0.92620206, 0.5520727 ],\n",
              "       [0.32353517, 0.4884625 , 0.36299706, 0.4034108 , 0.3980208 ,\n",
              "        0.29993683, 0.6758275 , 1.0000001 , 0.6768897 , 0.61469436,\n",
              "        0.75155985, 0.5577809 ],\n",
              "       [0.33085033, 0.4031633 , 0.35417843, 0.35402954, 0.36890098,\n",
              "        0.30385193, 0.7730411 , 0.6768897 , 1.0000002 , 0.72277844,\n",
              "        0.7781273 , 0.60181195],\n",
              "       [0.2717247 , 0.36106873, 0.26405597, 0.3217687 , 0.3174026 ,\n",
              "        0.21876109, 0.69483066, 0.61469436, 0.72277844, 0.99999976,\n",
              "        0.7090598 , 0.50095105],\n",
              "       [0.32900903, 0.42383695, 0.3212558 , 0.3496586 , 0.33768153,\n",
              "        0.2602014 , 0.92620206, 0.75155985, 0.7781273 , 0.7090598 ,\n",
              "        0.99999994, 0.5672187 ],\n",
              "       [0.20401588, 0.26584467, 0.21048519, 0.21814035, 0.28169265,\n",
              "        0.20053254, 0.5520727 , 0.5577809 , 0.60181195, 0.50095105,\n",
              "        0.5672187 , 0.9999997 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Retrieval\n",
        "\n",
        "To introduce the problem, we take some example Chinese paragraphs to use as queries and English paragraphs to use as candidate results to search through."
      ],
      "metadata": {
        "id": "2EA4rLHyG7ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_articles = [r['text'] for r in articles if r['lang'] == 'zh']\n",
        "result_articles = [r['text'] for r in articles if r['lang'] == 'en']"
      ],
      "metadata": {
        "id": "WLp0M_LfdurI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the example clearer, we will use different numbers of queries and results."
      ],
      "metadata": {
        "id": "kpZZQxMLHI8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qembed = labse.encode(query_articles[0:200])\n",
        "rembed = labse.encode(result_articles[0:500])"
      ],
      "metadata": {
        "id": "pj8-CRnneWp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiplying the query embeddings by the result embeddings, we get a $200 \\times 500$ queries-by-results matrix."
      ],
      "metadata": {
        "id": "hbW7NlYjHTsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim = qembed @ rembed.T\n",
        "sim.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlq2c0uUerDg",
        "outputId": "9fd809a6-87fb-4387-c7d4-0d5be64ccdc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 500)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use numpy's `argmax` function along the second dimension (`axis=1`) to get the index of the top result for each query."
      ],
      "metadata": {
        "id": "c2zqzNYsHi3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "argmax = np.argmax(sim, axis=1)\n",
        "argmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyt7Qjg4HjW1",
        "outputId": "aaffafa9-babd-4e56-b64b-024c7f2435e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([454,   1,   2, 282, 410, 120,  13,   7, 162,   9,  10,  11,  12,\n",
              "        13,  14,  15, 153, 212, 247,  19,  20,  21,  22,  23, 372, 372,\n",
              "        26,  27,  28,  29,  82,  31,  32,  33,  34,  35,  36,  66,  93,\n",
              "        39, 397,  41, 383,  83,  44,  45,  46,  47,  48,  49,  50, 245,\n",
              "       296, 139,  54,  55,  56,  57,  58,  59, 266,  61,  62, 171,  64,\n",
              "        31,  29,  67,  68,  69,  70,  71, 242, 372,  74,  75,  76,  77,\n",
              "       253,  79,  80,  81,  82,  83, 160,  85,  86, 492,  88,  17,  90,\n",
              "        91,  92,  93,  94,  95,  96,  97,  98,  99,  32, 101, 102, 103,\n",
              "       104, 307, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
              "       117, 118, 119, 120,  82, 122, 123, 124,  14, 126, 127, 128, 129,\n",
              "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
              "       273, 144, 424, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
              "       156, 157, 158, 159, 160, 161, 162, 163, 120, 165, 166, 167, 171,\n",
              "       169, 170, 171, 147, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
              "       182, 183, 184, 185, 186, 173, 188, 189, 214, 193, 192, 193, 194,\n",
              "       378, 196, 197, 198, 199])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the query and result documents are in the same order, matching Chinese and English documents have the same index. This allows us to compute the accuracy, or &ldquo;recall at 1&rdquo;, of Chinese-to-English retrieval."
      ],
      "metadata": {
        "id": "fVmtHxLgJMVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum([a==b for (a, b) in zip(range(len(argmax)), argmax)])/len(argmax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkLGfwr8nkAL",
        "outputId": "b961f60a-ea15-475b-d03b-10c035db4f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.785)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your first task is to compute the recall at 1 for Arabic, Chinese, French, German, and Greek query documents matching English documents. Use the first 1000 English documents as the candidates you will search through."
      ],
      "metadata": {
        "id": "uNdZ_w5PJ5FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = labse.encode(result_articles[0:1000])"
      ],
      "metadata": {
        "id": "LaX3-X3vKBek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each of the other five languages, construct embeddings for the first 1000 documents and measure how often the most similar English document is the matching one."
      ],
      "metadata": {
        "id": "R2vTuvDqKmZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute and print the recall at 1 for X-English retrieval\n",
        "# where X \\in {ar,de,el,fr,zh}\n",
        "ar_query_articles = [r['text'] for r in articles if r['lang'] == 'ar']\n",
        "ar_qembed = labse.encode(ar_query_articles[0:200])\n",
        "ar_sim = ar_qembed @ candidates.T\n",
        "ar_argmax = np.argmax(ar_sim, axis=1)\n",
        "ae_recall_at_1 = sum([a==b for (a, b) in zip(range(len(ar_argmax)), ar_argmax)])/len(ar_argmax)\n",
        "print(\"Recall@1 for Arabic-English:\", ae_recall_at_1)\n",
        "\n",
        "de_query_articles = [r['text'] for r in articles if r['lang'] == 'de']\n",
        "de_qembed = labse.encode(de_query_articles[0:200])\n",
        "de_sim = de_qembed @ candidates.T\n",
        "de_argmax = np.argmax(de_sim, axis=1)\n",
        "de_recall_at_1 = sum([a==b for (a, b) in zip(range(len(de_argmax)), de_argmax)])/len(de_argmax)\n",
        "print(\"Recall@1 for German-English:\", de_recall_at_1)\n",
        "\n",
        "el_query_articles = [r['text'] for r in articles if r['lang'] == 'el']\n",
        "el_qembed = labse.encode(el_query_articles[0:200])\n",
        "el_sim = el_qembed @ candidates.T\n",
        "el_argmax = np.argmax(el_sim, axis=1)\n",
        "el_recall_at_1 = sum([a==b for (a, b) in zip(range(len(el_argmax)), el_argmax)])/len(el_argmax)\n",
        "print(\"Recall@1 for Greek-English:\", el_recall_at_1)\n",
        "\n",
        "fr_query_articles = [r['text'] for r in articles if r['lang'] == 'fr']\n",
        "fr_qembed = labse.encode(fr_query_articles[0:200])\n",
        "fr_sim = fr_qembed @ candidates.T\n",
        "fr_argmax = np.argmax(fr_sim, axis=1)\n",
        "fr_recall_at_1 = sum([a==b for (a, b) in zip(range(len(fr_argmax)), fr_argmax)])/len(fr_argmax)\n",
        "print(\"Recall@1 for French-English:\", fr_recall_at_1)\n",
        "\n",
        "zh_query_articles = [r['text'] for r in articles if r['lang'] == 'zh']\n",
        "zh_qembed = labse.encode(zh_query_articles[0:200])\n",
        "zh_sim = zh_qembed @ candidates.T\n",
        "zh_argmax = np.argmax(zh_sim, axis=1)\n",
        "zh_recall_at_1 = sum([a==b for (a, b) in zip(range(len(zh_argmax)), zh_argmax)])/len(zh_argmax)\n",
        "print(\"Recall@1 for Chinese-English:\", zh_recall_at_1)"
      ],
      "metadata": {
        "id": "MP-RwjnaLPMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12ef046-cbec-4ac8-c171-b334437f388e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@1 for Arabic-English: 0.88\n",
            "Recall@1 for German-English: 0.86\n",
            "Recall@1 for Greek-English: 0.905\n",
            "Recall@1 for French-English: 0.86\n",
            "Recall@1 for Chinese-English: 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We often use retrieved documents to provide extra context to a language model. In that case, we might retrieve more than one document per query to increase the likelihood that useful documents are in the top $k$. For each of the five non-English languages, write code to evaluate the **recall at k** (R@k), i.e., the proportion of queries for which the correct document was anywhere in the top k results."
      ],
      "metadata": {
        "id": "LMbZrLYfLdrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Write a function to compute recall at k\n",
        "def recall_at_k(lang_code, k, query_size, candidate_size):\n",
        "    query_articles = [r['text'] for r in articles if r['lang'] == lang_code][:query_size]\n",
        "    qembed = labse.encode(query_articles)\n",
        "\n",
        "    candidate_articles = [r['text'] for r in articles if r['lang'] == 'en'][:candidate_size]\n",
        "    cembed = labse.encode(candidate_articles)\n",
        "\n",
        "    sim = qembed @ cembed.T\n",
        "\n",
        "    topk = np.argsort(-sim, axis=1)[:, :k]\n",
        "\n",
        "    hits = [i in row for i, row in enumerate(topk)]\n",
        "\n",
        "    recall = sum(hits) / len(hits)\n",
        "\n",
        "    print(f\"Recall@{k} for {lang_code}-English:\", round(recall, 3))\n",
        "    return recall\n",
        "\n"
      ],
      "metadata": {
        "id": "EYLicLyKMWGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute and print recall at 5 and recall at 10 for X-English retrieval\n",
        "# where X \\in {ar,de,el,fr,zh}\n",
        "\n",
        "recall_at_k('ar', 5, 200, 1000)\n",
        "recall_at_k('de', 5, 200, 1000)\n",
        "recall_at_k('el', 5, 200, 1000)\n",
        "recall_at_k('fr', 5, 200, 1000)\n",
        "recall_at_k('zh', 5, 200, 1000)"
      ],
      "metadata": {
        "id": "_GqeHauvhKYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fbcf21a-782e-4313-d15b-1de072ab3197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@5 for ar-English: 0.95\n",
            "Recall@5 for de-English: 0.92\n",
            "Recall@5 for el-English: 0.945\n",
            "Recall@5 for fr-English: 0.925\n",
            "Recall@5 for zh-English: 0.85\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different Retrieval Strategies\n",
        "\n",
        "**TODO**: Not all languages perform equally well using the LaBSE model. Your task is to find an alternative retrieval method that _improves performance for at least one language_ while _not degrading performance for other languages_.\n",
        "\n",
        "You are free to use any open encoder or generative models available on huggingface. Here are three ideas to get you started. You only need to implement one improvement, although you may keep other dead-ends in the notebook.\n",
        "\n",
        "1. Find other embedding models on huggingface that work better for, e.g., Chinese, while maintaining performance on the other languages.\n",
        "1. LaBSE was trained on translation pairs, but Wikipedia articles are not necessarily translations of each other. Use the remaining articles in the dataset to fine-tune LaBSE (or another model). [This huggingface guide to fine-tuning sentence embeddings](https://huggingface.co/blog/train-sentence-transformers) may be helpful.\n",
        "1. Instead of using embeddings, you could use a generative model to try to directly output the title of the English article given the foreign-language title and article. This approach is known as [generative retrieval](https://arxiv.org/abs/2404.14851).\n",
        "\n",
        "What you try is up to you. Describe your approach and use the recall at k function above to evaluate your results."
      ],
      "metadata": {
        "id": "T1YvHVGdjVS8"
      }
    }
  ]
}
